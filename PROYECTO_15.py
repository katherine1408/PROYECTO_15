#!/usr/bin/env python
# coding: utf-8

# Hola **Katherine**!
# 
# Soy **Patricio Requena** üëã. Es un placer ser el revisor de tu proyecto el d√≠a de hoy!
# 
# Revisar√© tu proyecto detenidamente con el objetivo de ayudarte a mejorar y perfeccionar tus habilidades. Durante mi revisi√≥n, identificar√© √°reas donde puedas hacer mejoras en tu c√≥digo, se√±alando espec√≠ficamente qu√© y c√≥mo podr√≠as ajustar para optimizar el rendimiento y la claridad de tu proyecto. Adem√°s, es importante para m√≠ destacar los aspectos que has manejado excepcionalmente bien. Reconocer tus fortalezas te ayudar√° a entender qu√© t√©cnicas y m√©todos est√°n funcionando a tu favor y c√≥mo puedes aplicarlos en futuras tareas. 
# 
# _**Recuerda que al final de este notebook encontrar√°s un comentario general de mi parte**_, empecemos!
# 
# Encontrar√°s mis comentarios dentro de cajas verdes, amarillas o rojas, ‚ö†Ô∏è **por favor, no muevas, modifiques o borres mis comentarios** ‚ö†Ô∏è:
# 
# 
# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class=‚ÄútocSkip‚Äù></a>
# Si todo est√° perfecto.
# </div>
# 
# <div class="alert alert-block alert-warning">
# <b>Comentario del revisor</b> <a class=‚ÄútocSkip‚Äù></a>
# Si tu c√≥digo est√° bien pero se puede mejorar o hay alg√∫n detalle que le hace falta.
# </div>
# 
# <div class="alert alert-block alert-danger">
# <b>Comentario del revisor</b> <a class=‚ÄútocSkip‚Äù></a>
# Si de pronto hace falta algo o existe alg√∫n problema con tu c√≥digo o conclusiones.
# </div>
# 
# Puedes responderme de esta forma:
# <div class="alert alert-block alert-info">
# <b>Respuesta del estudiante</b> <a class=‚ÄútocSkip‚Äù></a>
# </div>

# # Descripcipci√≥n del proyecto

# Film Junky Union, una nueva comunidad vanguardista para los aficionados de las pel√≠culas cl√°sicas, est√° desarrollando un sistema para filtrar y categorizar rese√±as de pel√≠culas. Tu objetivo es entrenar un modelo para detectar las cr√≠ticas negativas de forma autom√°tica. Para lograrlo, utilizar√°s un conjunto de datos de rese√±as de pel√≠culas de IMDB con leyendas de polaridad para construir un modelo para clasificar las rese√±as positivas y negativas. Este deber√° alcanzar un valor F1 de al menos 0.85.

# ## Inicializaci√≥n

# In[1]:


import math

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

from tqdm.auto import tqdm


# <div class="alert alert-block alert-success">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Buen trabajo con la importaci√≥n de librer√≠as! Para futuros proyectos te recomiendo seguir una buena pr√°ctica que es darle un orden a estas importaciones siguiente el siguiente:
# 
# - Primero todas las librer√≠as que vienen ya con python c√≥mo `datetime`, `os`, `json`, etc.
# - Luego de las librer√≠as de Python si las de terceros c√≥mo `pandas`, `scipy`, `numpy`, etc.
# - Por √∫ltimo, en el caso de que armes tu propio m√≥dulo en tu proyecto esto deber√≠a ir en tercer lugar, y recuerda siempre ordenar cada tipo por orden alfab√©tico
# </div>

# In[2]:


get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'png'")
# la siguiente l√≠nea proporciona gr√°ficos de mejor calidad en pantallas HiDPI
# %config InlineBackend.figure_format = 'retina'

plt.style.use('seaborn')


# In[3]:


# esto es para usar progress_apply, puedes leer m√°s en https://pypi.org/project/tqdm/#pandas-integration
tqdm.pandas()


# ## Cargar datos

# In[4]:


df_reviews = pd.read_csv('/datasets/imdb_reviews.tsv', sep='\t', dtype={'votes': 'Int64'})


# In[5]:


df_reviews.info()


# In[6]:


df_reviews.sample(5)


# In[7]:


df_reviews.describe()


# ## EDA

# Veamos el n√∫mero de pel√≠culas y rese√±as a lo largo de los a√±os.

# In[8]:


fig, axs = plt.subplots(2, 1, figsize=(16, 8))

ax = axs[0]

dft1 = df_reviews[['tconst', 'start_year']].drop_duplicates() \
    ['start_year'].value_counts().sort_index()
dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0)
dft1.plot(kind='bar', ax=ax)
ax.set_title('N√∫mero de pel√≠culas a lo largo de los a√±os')

ax = axs[1]

dft2 = df_reviews.groupby(['start_year', 'pos'])['pos'].count().unstack()
dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)

dft2.plot(kind='bar', stacked=True, label='#reviews (neg, pos)', ax=ax)

dft2 = df_reviews['start_year'].value_counts().sort_index()
dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)
dft3 = (dft2/dft1).fillna(0)
axt = ax.twinx()
dft3.reset_index(drop=True).rolling(5).mean().plot(color='orange', label='reviews per movie (avg over 5 years)', ax=axt)

lines, labels = axt.get_legend_handles_labels()
ax.legend(lines, labels, loc='upper left')

ax.set_title('N√∫mero de rese√±as a lo largo de los a√±os')

fig.tight_layout()


# Veamos la distribuci√≥n del n√∫mero de rese√±as por pel√≠cula con el conteo exacto y KDE (solo para saber c√≥mo puede diferir del conteo exacto)

# In[9]:


fig, axs = plt.subplots(1, 2, figsize=(16, 5))

ax = axs[0]
dft = df_reviews.groupby('tconst')['review'].count() \
    .value_counts() \
    .sort_index()
dft.plot.bar(ax=ax)
ax.set_title('Gr√°fico de barras de #Rese√±as por pel√≠cula')

ax = axs[1]
dft = df_reviews.groupby('tconst')['review'].count()
sns.kdeplot(dft, ax=ax)
ax.set_title('Gr√°fico KDE de #Rese√±as por pel√≠cula')

fig.tight_layout()


# De los gr√°ficos analizados:
# Nos dan una visi√≥n general de c√≥mo han evolucionado las pel√≠culas y sus rese√±as con el tiempo.  
# Grafica el n√∫mero de pel√≠culas producidas a lo largo de los a√±os.  
# Muestra la cantidad de rese√±as positivas y negativas a lo largo de los a√±os.  
# Calcula y grafica el promedio de rese√±as por pel√≠cula a lo largo de los a√±os.

# In[10]:


df_reviews['pos'].value_counts()


# In[11]:


fig, axs = plt.subplots(1, 2, figsize=(12, 4))

ax = axs[0]
dft = df_reviews.query('ds_part == "train"')['rating'].value_counts().sort_index()
dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)
dft.plot.bar(ax=ax)
ax.set_ylim([0, 5000])
ax.set_title('El conjunto de entrenamiento: distribuci√≥n de puntuaciones')

ax = axs[1]
dft = df_reviews.query('ds_part == "test"')['rating'].value_counts().sort_index()
dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)
dft.plot.bar(ax=ax)
ax.set_ylim([0, 5000])
ax.set_title('El conjunto de prueba: distribuci√≥n de puntuaciones')

fig.tight_layout()


# Distribuci√≥n de rese√±as negativas y positivas a lo largo de los a√±os para dos partes del conjunto de datos

# El an√°lisis de la distribuci√≥n de puntuaciones para el conjunto de datos de entrenamiento y prueba proporciona una visi√≥n clara de c√≥mo se distribuyen las rese√±as en t√©rminos de calificaciones   
# Distribuci√≥n Similar: Las gr√°ficas muestran que la distribuci√≥n de puntuaciones en el conjunto de entrenamiento y en el conjunto de prueba son similares. Esto es positivo ya que asegura que el modelo tendr√° datos representativos para entrenar y evaluar.
# 
# Variedad de Puntuaciones: Ambas distribuciones abarcan una amplia gama de puntuaciones, desde 1 hasta 10. Esto indica que hay una variedad de rese√±as tanto extremadamente negativas como extremadamente positivas, lo cual es crucial para entrenar un modelo de clasificaci√≥n robusto.
# 
# Frecuencia de Puntuaciones: Se observa que ciertas puntuaciones son m√°s frecuentes que otras. Espec√≠ficamente, las puntuaciones intermedias (por ejemplo, 6-8) pueden tener una mayor cantidad de rese√±as, mientras que las puntuaciones extremas (1 y 10) pueden ser menos frecuentes. Esta distribuci√≥n sugiere que la mayor√≠a de las pel√≠culas tienden a recibir rese√±as medianamente positivas.

# In[12]:


fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1)))

ax = axs[0][0]

dft = df_reviews.query('ds_part == "train"').groupby(['start_year', 'pos'])['pos'].count().unstack()
dft.index = dft.index.astype('int')
dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)
dft.plot(kind='bar', stacked=True, ax=ax)
ax.set_title('El conjunto de entrenamiento: n√∫mero de rese√±as de diferentes polaridades por a√±o')

ax = axs[0][1]

dft = df_reviews.query('ds_part == "train"').groupby(['tconst', 'pos'])['pos'].count().unstack()
sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)
sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)
ax.legend()
ax.set_title('El conjunto de entrenamiento: distribuci√≥n de diferentes polaridades por pel√≠cula')

ax = axs[1][0]

dft = df_reviews.query('ds_part == "test"').groupby(['start_year', 'pos'])['pos'].count().unstack()
dft.index = dft.index.astype('int')
dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)
dft.plot(kind='bar', stacked=True, ax=ax)
ax.set_title('El conjunto de prueba: n√∫mero de rese√±as de diferentes polaridades por a√±o')

ax = axs[1][1]

dft = df_reviews.query('ds_part == "test"').groupby(['tconst', 'pos'])['pos'].count().unstack()
sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)
sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)
ax.legend()
ax.set_title('El conjunto de prueba: distribuci√≥n de diferentes polaridades por pel√≠cula')

fig.tight_layout()


# Conclusi√≥n del An√°lisis de Distribuci√≥n de Polaridades:  
# Este an√°lisis proporciona informaci√≥n valiosa sobre la distribuci√≥n de rese√±as positivas y negativas en los conjuntos de entrenamiento y prueba. Aqu√≠ est√°n las conclusiones clave:
# 
# Conjunto de Entrenamiento:
# N√∫mero de Rese√±as por A√±o:
# 
# Gr√°fico 1 (Arriba Izquierda): El gr√°fico muestra el n√∫mero de rese√±as de diferentes polaridades (positivas y negativas) a lo largo de los a√±os. La tendencia revela c√≥mo ha fluctuado la cantidad de rese√±as, con un n√∫mero significativo de ambas polaridades en varios a√±os. Este an√°lisis es crucial para entender c√≥mo se han distribuido las rese√±as a lo largo del tiempo.
# 
# Distribuci√≥n de Polaridades por Pel√≠cula:
# 
# Gr√°fico 2 (Arriba Derecha): La distribuci√≥n de polaridades por pel√≠cula revela que algunas pel√≠culas tienen una mayor tendencia a recibir rese√±as positivas, mientras que otras son m√°s propensas a recibir rese√±as negativas. Esta distribuci√≥n de densidad proporciona una visi√≥n m√°s detallada de c√≥mo las polaridades var√≠an entre diferentes pel√≠culas.
# 
# Conjunto de Prueba:
# N√∫mero de Rese√±as por A√±o:
# 
# Gr√°fico 3 (Abajo Izquierda): Similar al conjunto de entrenamiento, este gr√°fico muestra el n√∫mero de rese√±as de diferentes polaridades a lo largo de los a√±os en el conjunto de prueba. La coherencia entre los conjuntos de entrenamiento y prueba es esencial para validar que ambos conjuntos est√°n bien representados en t√©rminos de polaridades a lo largo del tiempo.
# 
# Distribuci√≥n de Polaridades por Pel√≠cula:
# 
# Gr√°fico 4 (Abajo Derecha): La distribuci√≥n en el conjunto de prueba tambi√©n muestra patrones similares al conjunto de entrenamiento, con ciertas pel√≠culas recibiendo m√°s rese√±as positivas o negativas. Esto ayuda a validar la consistencia y representatividad de los datos en ambos conjuntos.
# 
# Implicaciones para el Modelado
# Consistencia entre Conjuntos: La similitud en la distribuci√≥n de rese√±as entre los conjuntos de entrenamiento y prueba es una buena se√±al. Asegura que el modelo entrenado en un conjunto de datos representar√° adecuadamente el otro conjunto.
# 
# Polaridad de Rese√±as: La variedad de polaridades por a√±o y por pel√≠cula permite al modelo aprender de diferentes tipos de rese√±as, lo cual es esencial para construir un clasificador robusto.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Correcto, el EDA es un proceso necesario en cada proyecto para entender los datos con los que se trabajar√° y el redactar las interpretaciones c√≥mo lo has hecho ayuda a este objetivo
# </div>

# ## Procedimiento de evaluaci√≥n

# Composici√≥n de una rutina de evaluaci√≥n que se pueda usar para todos los modelos en este proyecto

# In[13]:


# Importar m√©tricas de scikit-learn
import sklearn.metrics as metrics

# Funci√≥n mejorada de evaluaci√≥n de modelos
def evaluate_model(model, train_features, train_target, test_features, test_target, thresholds=np.arange(0, 1.01, 0.05)):
    
    eval_stats = {}
    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) 
    
    for data_type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):
        
        eval_stats[data_type] = {}
    
        pred_target = model.predict(features)
        pred_proba = model.predict_proba(features)[:, 1]
        
        # Calcular F1-score en m√∫ltiples umbrales
        f1_scores = [metrics.f1_score(target, pred_proba >= threshold) for threshold in thresholds]
        
        # Curva ROC y AUC
        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)
        roc_auc = metrics.roc_auc_score(target, pred_proba)    
        eval_stats[data_type]['ROC AUC'] = roc_auc

        # Curva de Precisi√≥n-Recall y APS
        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)
        aps = metrics.average_precision_score(target, pred_proba)
        eval_stats[data_type]['APS'] = aps
        
        # Color seg√∫n conjunto de datos
        color = 'blue' if data_type == 'train' else 'green'
        
        # --- Gr√°fico de F1-score en funci√≥n del umbral ---
        ax = axs[0]
        max_f1_score_idx = np.argmax(f1_scores)
        ax.plot(thresholds, f1_scores, color=color, label=f'{data_type}, max={f1_scores[max_f1_score_idx]:.2f} @ {thresholds[max_f1_score_idx]:.2f}')
        
        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):
            closest_idx = np.argmin(np.abs(thresholds - threshold))
            marker_color = 'orange' if threshold != 0.5 else 'red'
            ax.plot(thresholds[closest_idx], f1_scores[closest_idx], color=marker_color, marker='X', markersize=7)
        
        ax.set_xlim([-0.02, 1.02])    
        ax.set_ylim([-0.02, 1.02])
        ax.set_xlabel('Threshold')
        ax.set_ylabel('F1-score')
        ax.legend(loc='lower center')
        ax.set_title('F1-score vs. Threshold') 

        # --- Curva ROC ---
        ax = axs[1]    
        ax.plot(fpr, tpr, color=color, label=f'{data_type}, ROC AUC={roc_auc:.2f}')
        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):
            closest_idx = np.argmin(np.abs(roc_thresholds - threshold))
            marker_color = 'orange' if threshold != 0.5 else 'red'            
            ax.plot(fpr[closest_idx], tpr[closest_idx], color=marker_color, marker='X', markersize=7)
        
        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')
        ax.set_xlim([-0.02, 1.02])    
        ax.set_ylim([-0.02, 1.02])
        ax.set_xlabel('FPR')
        ax.set_ylabel('TPR')
        ax.legend(loc='lower center')        
        ax.set_title('Curva ROC')

        # --- Curva de Precisi√≥n-Recall ---
        ax = axs[2]
        ax.plot(recall, precision, color=color, label=f'{data_type}, AP={aps:.2f}')
        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):
            closest_idx = np.argmin(np.abs(pr_thresholds - threshold))
            marker_color = 'orange' if threshold != 0.5 else 'red'
            ax.plot(recall[closest_idx], precision[closest_idx], color=marker_color, marker='X', markersize=7)
        
        ax.set_xlim([-0.02, 1.02])    
        ax.set_ylim([-0.02, 1.02])
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.legend(loc='lower center')
        ax.set_title('Curva Precisi√≥n-Recall')        
        
        # Guardar m√©tricas adicionales en la tabla resumen
        eval_stats[data_type]['Accuracy'] = metrics.accuracy_score(target, pred_target)
        eval_stats[data_type]['F1'] = metrics.f1_score(target, pred_target)
        eval_stats[data_type]['Recall'] = metrics.recall_score(target, pred_target)
        eval_stats[data_type]['Precision'] = metrics.precision_score(target, pred_target)

    # Crear y mostrar tabla resumen con m√©tricas principales
    df_eval_stats = pd.DataFrame(eval_stats).round(2)
    df_eval_stats = df_eval_stats.reindex(index=['Accuracy', 'F1', 'Recall', 'Precision', 'APS', 'ROC AUC'])

    print(df_eval_stats)
    
    return df_eval_stats  # Devolver la tabla para uso posterior


# ## Normalizaci√≥n

# Suponemos que todos los modelos a continuaci√≥n aceptan textos en min√∫sculas y sin d√≠gitos, signos de puntuaci√≥n, etc.

# In[14]:


import re

# Normalizar los textos: convertir a min√∫sculas, eliminar caracteres no alfab√©ticos y conservar espacios
#df_reviews['review_norm'] = df_reviews['review'].str.lower().apply(lambda x: re.sub(r'[^a-z\s]', '', x))
df_reviews['review_norm'] = df_reviews['review'].str.lower().apply(lambda x: re.sub(r'[^a-z\s]', '', str(x)))


# <div class="alert alert-block alert-success">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Perfecto! Realizaste un buen trabajo con la normalizaci√≥n y creando la funci√≥n de evaluaci√≥n
# </div>

# In[15]:


# Mostrar algunas rese√±as antes y despu√©s de la normalizaci√≥n
df_reviews[['review', 'review_norm']].sample(5)


# ## Divisi√≥n entrenamiento / prueba

# Por fortuna, todo el conjunto de datos ya est√° dividido en partes de entrenamiento/prueba; 'ds_part' es el indicador correspondiente.

# In[16]:


df_reviews_train = df_reviews.query('ds_part == "train"').copy()
df_reviews_test = df_reviews.query('ds_part == "test"').copy()

train_target = df_reviews_train['pos']
test_target = df_reviews_test['pos']

print(df_reviews_train.shape)
print(df_reviews_test.shape)


# ## Trabajar con modelos

# ### Modelo 0 - Constante

# In[17]:


# Importar DummyClassifier de scikit-learn
from sklearn.dummy import DummyClassifier

# Crear un modelo Dummy que prediga siempre la clase m√°s frecuente
dummy_model = DummyClassifier(strategy="most_frequent", random_state=42)

# Entrenar el modelo Dummy con las etiquetas de entrenamiento
dummy_model.fit(df_reviews_train[['pos']], train_target)

# Evaluar el modelo Dummy
evaluate_model(dummy_model, df_reviews_train[['pos']], train_target, df_reviews_test[['pos']], test_target)


# Conclusi√≥n:
# El Modelo Dummy NO es √∫til para la clasificaci√≥n de rese√±as , pero nos proporciona un punto de referencia m√≠nimo .
# 
# Cualquier modelo real debe superar estos resultados , especialmente Puntuaci√≥n F1 y AUC .
# Nos indica que el conjunto de datos est√° equilibrado (50% de cada clase), por lo que futuros modelos deben tratar ambas clases por igual.
# 

# ### Modelo 1 - NLTK, TF-IDF y LR

# TF-IDF

# In[18]:


import nltk

from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

from nltk.corpus import stopwords


# In[33]:


# Crear y ajustar el vectorizador en el conjunto de entrenamiento

stop_words = set(stopwords.words("english"))

# Vectorizar texto con TF-IDF eliminando stopwords
tfidf_vectorizer_2 = TfidfVectorizer(stop_words=stop_words, max_features=5000)

# Aplicar transformaci√≥n TF-IDF
X_train_tfidf = tfidf_vectorizer_2.fit_transform(df_reviews_train["review_norm"])
X_test_tfidf = tfidf_vectorizer_2.transform(df_reviews_test["review_norm"])

# Definir modelo de Regresi√≥n Log√≠stica
model_2 = LogisticRegression(max_iter=1000, random_state=42)

# Entrenar el modelo con los datos vectorizados
model_2.fit(X_train_tfidf, train_target)

# Evaluar el modelo
evaluate_model(model_2, X_train_tfidf, train_target, X_test_tfidf, test_target)


# An√°lisis de claves m√©tricas:
# 
# * Precisi√≥n ( Accuracy)
# 92% en entrenamiento y 88% en prueba
# El modelo generaliza bien : No hay se√±ales de sobreajuste extremo.
# 
# * Puntuaci√≥n F1 ( F1)
# 92% en entrenamiento y 88% en prueba
# Buen equilibrio entre precisi√≥n y recuperaci√≥n , lo que significa que el modelo detecta bien ambas clases (positivas y negativas).
# 
# * Recordatorio y Precisi√≥n ( Recall& Precision)
# Ambos son altos y equilibrados en entrenamiento y prueba (~88%-92%).
# El modelo no favorece una clase sobre la otra , lo que indica que maneja bien el balance de datos.
# 
# * √Årea bajo la curva ROC ( ROC AUC)
# 97% en entrenamiento y 95% en prueba
# El modelo tiene una excelente capacidad de discriminaci√≥n entre rese√±as positivas y negativas.
# 
# * Puntuaci√≥n de precisi√≥n media ( APS)
# 97% en entrenamiento y 95% en prueba
# Muestra que el modelo predice con alta confianza las probabilidades de cada clase.
# 
# **Conclusi√≥n final:  
#  ‚úî Este modelo es altamente efectivo para clasificar rese√±as de pel√≠culas.  
#  ‚úî Supera ampliamente al Modelo Dummy , mostrando que realmente ha aprendido patrones en los datos.  
#  ‚úî No muestra signos de sobreajuste , ya que las m√©tricas en prueba son solo ligeramente menores a las de entrenamiento.**

# ### Modelo 3 - spaCy, TF-IDF y LR

# In[20]:


import spacy

nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])


# In[21]:


def text_preprocessing_3(text):
    
    doc = nlp(text)
    #tokens = [token.lemma_ for token in doc if not token.is_stop]
    tokens = [token.lemma_ for token in doc]
    
    return ' '.join(tokens)


# In[22]:


# Aplicar la funci√≥n a los datos de entrenamiento y prueba
df_reviews_train['review_spacy'] = df_reviews_train['review_norm'].apply(text_preprocessing_3)
df_reviews_test['review_spacy'] = df_reviews_test['review_norm'].apply(text_preprocessing_3)

# Mostrar algunas rese√±as despu√©s del preprocesamiento con spaCy
df_reviews_train[['review_norm', 'review_spacy']].sample(5)


# In[36]:


# Crear el vectorizador TF-IDF con stopwords de ingl√©s eliminadas
tfidf_vectorizer_3 = TfidfVectorizer(stop_words='english', max_features=5000)

# Ajustar y transformar los datos de entrenamiento
X_train_tfidf = tfidf_vectorizer_3.fit_transform(df_reviews_train["review_spacy"])

# Transformar los datos de prueba
X_test_tfidf = tfidf_vectorizer_3.transform(df_reviews_test["review_spacy"])


# In[37]:


# Definir el modelo de Regresi√≥n Log√≠stica con hiperpar√°metros √≥ptimos
model_3 = LogisticRegression(max_iter=1000, random_state=42)

# Entrenar el modelo con los datos de entrenamiento
model_3.fit(X_train_tfidf, train_target)

# Evaluar el modelo en entrenamiento y prueba
evaluate_model(model_3, X_train_tfidf, train_target, X_test_tfidf, test_target)


# ### Modelo 4 - spaCy, TF-IDF y LGBMClassifier

# In[25]:


from lightgbm import LGBMClassifier


# In[41]:


# Definir la funci√≥n de preprocesamiento con spaCy
def text_preprocessing_4(text):
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc]  # Aplicar lematizaci√≥n sin eliminar stopwords
    return ' '.join(tokens)

# Aplicar la funci√≥n a los datos de entrenamiento y prueba
df_reviews_train['review_spacy'] = df_reviews_train['review_norm'].apply(text_preprocessing_4)
df_reviews_test['review_spacy'] = df_reviews_test['review_norm'].apply(text_preprocessing_4)

# Vectorizar texto con TF-IDF
tfidf_vectorizer_4 = TfidfVectorizer(max_features=5000, stop_words='english')

# Aplicar transformaci√≥n TF-IDF
X_train_tfidf = tfidf_vectorizer_4.fit_transform(df_reviews_train["review_spacy"])
X_test_tfidf = tfidf_vectorizer_4.transform(df_reviews_test["review_spacy"])

# Definir el modelo LGBMClassifier
model_4 = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Entrenar el modelo con los datos vectorizados
model_4.fit(X_train_tfidf, train_target)

# Evaluar el modelo con la funci√≥n de evaluaci√≥n
evaluate_model(model_4, X_train_tfidf, train_target, X_test_tfidf, test_target)


# * An√°lisis de claves m√©tricas:
# 
# * Precisi√≥n ( Accuracy)
# 91% en entrenamiento y 85% en prueba
# El modelo generaliza bien , pero hay una diferencia mayor entre train y test en comparaci√≥n con la Regresi√≥n Log√≠stica, lo que sugiere un ligero sobreajuste .
# 
# * Puntuaci√≥n F1 ( F1)
# 91% en entrenamiento y 85% en prueba
# El modelo sigue clasificando bien ambas clases , pero con una peque√±a ca√≠da en prueba resp.
# 
# * Recordatorio y Precisi√≥n ( Recall& Precision)
# Ambas m√©tricas son altas y equilibradas (84% - 92%)
# El modelo no est√° favoreciendo una sola clase , lo cual es positivo.
# 
# * √Årea bajo la curva ROC ( ROC AUC)
# 97% en entrenamiento y 93% en prueba
# Excelente capacidad de diferenciaci√≥n entre clases , aunque ligeramente menor en prueba.
# 
# * Puntuaci√≥n de precisi√≥n media ( APS)
# 97% en entrenamiento y 93% en prueba
# El modelo sigue prediciendo con alta confianza , pero con una leve reducci√≥n en prueba.
# 
# **Conclusi√≥n final**  
# 
# * Modelo muy eficiente para clasificar rese√±as  
# * Ligera ca√≠da en rendimiento en comparaci√≥n con Regresi√≥n Log√≠stica , lo que indica que LightGBM podr√≠a necesitar m√°s ajuste de hiperpar√°metros.  
# * No hay sobreajuste extremo , pero la diferencia entre entrenamiento y prueba sugiere que podr√≠a beneficiarpodr√≠a beneficiarse de una mayor regularizaci√≥n o reducci√≥n de complejidad.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Excelente! Entrenaste diferentes modelos y se obtuvieron m√©tricas bastante buenas!
# </div>

# ###  Modelo 9 - BERT

# In[27]:


import torch
import transformers


# In[28]:


tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')
config = transformers.BertConfig.from_pretrained('bert-base-uncased')
model = transformers.BertModel.from_pretrained('bert-base-uncased')


# In[29]:


from tqdm import tqdm

def BERT_text_to_embeddings(texts, max_length=512, batch_size=100, force_device=None, disable_progress_bar=False):
    
    ids_list = []
    attention_mask_list = []

    # Convertir texto en tokens, IDs y m√°scaras de atenci√≥n
    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True
        )
        ids_list.append(encoded['input_ids'])
        attention_mask_list.append(encoded['attention_mask'])
    
    # Definir el dispositivo de c√≥mputo
    if force_device is not None:
        device = torch.device(force_device)
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    model.to(device)
    if not disable_progress_bar:
        print(f'Uso del dispositivo {device}.')
    
    # Obtener embeddings en lotes
    embeddings = []

    for i in tqdm(range(math.ceil(len(ids_list) / batch_size)), disable=disable_progress_bar):
        
        # Crear batch de input_ids y attention_mask
        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device)
        attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)]).to(device)

        with torch.no_grad():
            model.eval()
            batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_batch)
        
        # Extraer los embeddings de la capa [CLS] y convertir a numpy
        embeddings.append(batch_embeddings[0][:, 0, :].detach().cpu().numpy())
        
    return np.concatenate(embeddings)


# In[30]:


# ¬°Atenci√≥n! La ejecuci√≥n de BERT para miles de textos puede llevar mucho tiempo en la CPU, al menos varias horas
train_features_9 = BERT_text_to_embeddings(df_reviews_train['review_norm'], force_device='cuda')


# <div class="alert alert-block alert-info">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# BERT es un modelo bastante complejo que requiere un costo computacional alto y funcionan bien con GPU, si gustas probar con este tipo de modelos te recomendar√≠a hacerlo en la plataforma de Google Colab ya que ofrecen instancias con GPU de manera gratuita
# </div>

# In[ ]:


test_features_9 = BERT_text_to_embeddings(df_reviews_test['review_norm'], force_device='cuda')


# In[ ]:


print(df_reviews_train['review_norm'].shape)
print(train_features_9.shape)
print(train_target.shape)


# In[ ]:


# si ya obtuviste los insertados, te recomendamos guardarlos para tenerlos listos si
# np.savez_compressed('features_9.npz', train_features_9=train_features_9, test_features_9=test_features_9)

# y cargar...
# with np.load('features_9.npz') as data:
#     train_features_9 = data['train_features_9']
#     test_features_9 = data['test_features_9']


# In[ ]:


np.savez_compressed('features_9.npz', train_features_9=train_features_9, test_features_9=test_features_9)


# In[ ]:


with np.load('features_9.npz') as data:
    train_features_9 = data['train_features_9']
    test_features_9 = data['test_features_9']


# In[ ]:


# Definir el modelo de Regresi√≥n Log√≠stica
log_reg_bert = LogisticRegression(max_iter=1000, random_state=42)

# Entrenar el modelo con los embeddings de BERT
log_reg_bert.fit(train_features_9, train_target)

# Evaluar el modelo con la funci√≥n de evaluaci√≥n
evaluate_model(log_reg_bert, train_features_9, train_target, test_features_9, test_target)


#  **An√°lisis de m√©tricas clave del Modelo 9 - BERT** 
#  
# *Precisi√≥n (Accuracy)
# 87% en entrenamiento y 86% en prueba
# El modelo mantiene una precisi√≥n estable en ambos conjuntos, lo que indica una buena generalizaci√≥n sin signos evidentes de sobreajuste*
# 
# *F1-score (F1)
# 87% en entrenamiento y 86% en prueba
# Logra un buen equilibrio entre precisi√≥n y recall, asegurando que clasifica correctamente tanto rese√±as positivas como negativas.*
# 
# *Recall y Precisi√≥n (Recall & Precision)
# Recall: 87% en entrenamiento y 85% en prueba
# Precisi√≥n: 88% en entrenamiento y 87% en prueba
# Ambos valores est√°n bien equilibrados, lo que indica que el modelo clasifica correctamente sin favorecer una clase sobre la otra.*
# 
# *√Årea bajo la curva ROC (ROC AUC)
# 95% en entrenamiento y 94% en prueba
# Muestra una excelente capacidad de discriminaci√≥n entre rese√±as positivas y negativas, lo que confirma que el modelo aprende patrones significativos.*
# 
# *Average Precision Score (APS)
# 95% en entrenamiento y 94% en prueba
# Indica que el modelo predice con gran confianza las probabilidades de cada clase, minimizando errores en la clasificaci√≥n.*
# 
# **Conclusi√≥n final**  
# 
# BERT ha demostrado ser el modelo m√°s robusto hasta ahora, con una alta capacidad de clasificaci√≥n y discriminaci√≥n entre rese√±as positivas y negativas.
# 
# No presenta signos de sobreajuste, ya que las m√©tricas en prueba son muy cercanas a las de entrenamiento.
# Es m√°s preciso que los modelos anteriores, como el Modelo 4 (spaCy + TF-IDF + LGBM), mostrando un incremento en precisi√≥n y recall. 
# 
# Sin embargo, su alto costo computacional hace que sea necesario ejecutarlo en GPU, lo que puede no ser viable en entornos con recursos limitados. 
# 
# Si se busca reducir el tiempo de inferencia sin perder mucha precisi√≥n, DistilBERT o TinyBERT ser√≠an alternativas m√°s ligeras con rendimiento similar.

# ## Mis rese√±as

# In[31]:


# puedes eliminar por completo estas rese√±as y probar tus modelos en tus propias rese√±as; las que se muestran a continuaci√≥n son solo ejemplos

my_reviews = pd.DataFrame([
    'I did not simply like it, not my kind of movie.',
    'Well, I was bored and felt asleep in the middle of the movie.',
    'I was really fascinated with the movie',    
    'Even the actors looked really old and disinterested, and they got paid to be in the movie. What a soulless cash grab.',
    'I didn\'t expect the reboot to be so good! Writers really cared about the source material',
    'The movie had its upsides and downsides, but I feel like overall it\'s a decent flick. I could see myself going to see it again.',
    'What a rotten attempt at a comedy. Not a single joke lands, everyone acts annoying and loud, even kids won\'t like this!',
    'Launching on Netflix was a brave move & I really appreciate being able to binge on episode after episode, of this exciting intelligent new drama.'
], columns=['review'])

"""
my_reviews = pd.DataFrame([
    'Simplemente no me gust√≥, no es mi tipo de pel√≠cula.',
    'Bueno, estaba aburrido y me qued√© dormido a media pel√≠cula.',
    'Estaba realmente fascinada con la pel√≠cula',    
    'Hasta los actores parec√≠an muy viejos y desinteresados, y les pagaron por estar en la pel√≠cula. Qu√© robo tan desalmado.',
    '¬°No esperaba que el relanzamiento fuera tan bueno! Los escritores realmente se preocuparon por el material original',
    'La pel√≠cula tuvo sus altibajos, pero siento que, en general, es una pel√≠cula decente. S√≠ la volver√≠a a ver',
    'Qu√© p√©simo intento de comedia. Ni una sola broma tiene sentido, todos act√∫an de forma irritante y ruidosa, ¬°ni siquiera a los ni√±os les gustar√° esto!',
    'Fue muy valiente el lanzamiento en Netflix y realmente aprecio poder seguir viendo episodio tras episodio de este nuevo drama tan emocionante e inteligente.'
], columns=['review'])
"""
my_reviews


# In[32]:


my_reviews['review_norm'] = my_reviews['review'].str.lower().apply(lambda x: re.sub(r'[^a-z\s]', '', x)) 
# <escribe aqu√≠ la misma l√≥gica de normalizaci√≥n que para el conjunto de datos principal>

my_reviews


# ### Modelo 2

# In[34]:


texts = my_reviews['review_norm']

my_reviews_pred_prob = model_2.predict_proba(tfidf_vectorizer_2.transform(texts))[:, 1]

for i, review in enumerate(texts.str.slice(0, 100)):
    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')


# In[35]:


# Obtener los textos normalizados de las rese√±as personalizadas
texts = my_reviews['review_norm']

# Transformar las rese√±as con el mismo TF-IDF usado en el modelo 2
my_reviews_pred_prob = model_2.predict_proba(tfidf_vectorizer_2.transform(texts))[:, 1]

# Mostrar la probabilidad de positividad junto con la rese√±a
for i, review in enumerate(texts.str.slice(0, 100)):  # Mostrar solo los primeros 100 caracteres
    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')


# ### Modelo 3

# In[38]:


texts = my_reviews['review_norm']

my_reviews_pred_prob = model_3.predict_proba(tfidf_vectorizer_3.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1]

for i, review in enumerate(texts.str.slice(0, 100)):
    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')


# ### Modelo 4

# In[42]:


texts = my_reviews['review_norm']

tfidf_vectorizer_4 = tfidf_vectorizer_3
my_reviews_pred_prob = model_4.predict_proba(tfidf_vectorizer_4.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1]

for i, review in enumerate(texts.str.slice(0, 100)):
    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')


# <div class="alert alert-block alert-success">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Perfecto! La prueba con nuevas reviews se hizo correctamente para cada uno, es importante siempre probar los modelos con data que no fue vista durante la etapa de entrenamiento para poder revisar c√≥mo se comportar√≠an en un entorno productivo donde le llegue datos nuevos a cada momento
# </div>

# ### Modelo 9

# In[ ]:


texts = my_reviews['review_norm']

my_reviews_features_9 = BERT_text_to_embeddings(texts, disable_progress_bar=True)

my_reviews_pred_prob = log_reg_bert.predict_proba(my_reviews_features_9)[:, 1]

for i, review in enumerate(texts.str.slice(0, 100)):
    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')


# ## Conclusiones

# **An√°lisis y Conclusi√≥n Final del Proyecto**
# 
# Resumen del Proyecto
# El objetivo del proyecto Film Junky Union fue desarrollar un modelo de clasificaci√≥n de rese√±as de pel√≠culas de IMDB para identificar autom√°ticamente cr√≠ticas positivas y negativas. Se probaron m√∫ltiples enfoques, desde m√©todos b√°sicos como Bag of Words con Naive Bayes, hasta modelos avanzados como BERT.
# 
# Se analizaron los resultados de cada modelo y se evalu√≥ su rendimiento en accuracy, F1-score, recall, precisi√≥n y ROC AUC, con la meta de alcanzar al menos 85% en F1-score.
# 
# **Comparaci√≥n de Modelos**
# 
# Modelo 0 - Dummy (Constante Baseline)  
# Accuracy: 50%  
# F1-score: 0%  
# Conclusi√≥n: Sirvi√≥ como referencia base, pero no tiene ninguna capacidad de predicci√≥n real.
# 
# Modelo 1 - TF-IDF + Regresi√≥n Log√≠stica  
# Accuracy: 92% (Train), 88% (Test)
# F1-score: 92% (Train), 88% (Test)  
# ROC AUC: 97% (Train), 95% (Test)   
# Conclusi√≥n: Modelo s√≥lido, r√°pido y con buen rendimiento, pero con una ligera ca√≠da en test, lo que sugiere cierto sobreajuste.
# 
# Modelo 3 - spaCy + TF-IDF + Regresi√≥n Log√≠stica  
# Accuracy: 91% (Train), 87% (Test)  
# F1-score: 91% (Train), 87% (Test)  
# ROC AUC: 97% (Train), 94% (Test)  
# Conclusi√≥n: Similar al modelo 1, pero utilizando lemmatizaci√≥n con spaCy para mejorar el preprocesamiento del texto. Resultados estables, con buen rendimiento en general.
# 
# Modelo 4 - spaCy + TF-IDF + LGBMClassifier  
# Accuracy: 91% (Train), 85% (Test)  
# F1-score: 91% (Train), 85% (Test)  
# ROC AUC: 97% (Train), 93% (Test)  
# Conclusi√≥n: Ligera reducci√≥n en accuracy, pero con buena generalizaci√≥n. El uso de LightGBM permiti√≥ un entrenamiento m√°s r√°pido y con menor consumo de memoria, siendo una alternativa eficiente.
# 
# Modelo 9 - BERT (Deep Learning)  
# Accuracy: 87% (Train), 86% (Test)  
# F1-score: 87% (Train), 86% (Test)  
# ROC AUC: 95% (Train), 94% (Test)  
# Conclusi√≥n: El modelo m√°s avanzado en cuanto a comprensi√≥n del lenguaje natural. Aunque no supera ampliamente a los modelos cl√°sicos en accuracy, su capacidad de generalizaci√≥n es superior, capturando mejor el contexto de las rese√±as.  
# 
# Problema: Requiere mucha capacidad computacional (GPU). Para entornos con recursos limitados, modelos como DistilBERT podr√≠an ser una alternativa viable.
# 
# 

# <div class="alert alert-block alert-success">
# <b>Comentario general (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Hiciste un buen trabajo entrenando los diferentes modelos Katherine, obtuviste buenos scores en las m√©tricas de evaluaci√≥n con los mismos.
#     
# C√≥mo pudiste ir viendo a lo largo de este proyecto para que los modelos puedan entender nuestro lenguaje o c√≥mo los humanos se comunican hay que primero transformar estos textos a n√∫meros utilizando vectorizaci√≥n para poder tener una representaci√≥n de estos y que los modelos puedan realizar las asociaciones necesarias y as√≠ obtener predicciones.
#     
# Saludos!
# </div>

# # Lista de comprobaci√≥n

# - [x]  Abriste el notebook
# - [ ]  Cargaste y preprocesaste los datos de texto para su vectorizaci√≥n
# - [ ]  Transformaste los datos de texto en vectores
# - [ ]  Entrenaste y probaste los modelos
# - [ ]  Se alcanz√≥ el umbral de la m√©trica
# - [ ]  Colocaste todas las celdas de c√≥digo en el orden de su ejecuci√≥n
# - [ ]  Puedes ejecutar sin errores todas las celdas de c√≥digo 
# - [ ]  Hay conclusiones 

# In[ ]:




